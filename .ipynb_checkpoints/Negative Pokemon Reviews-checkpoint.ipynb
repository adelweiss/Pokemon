{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import re \n",
    "import string \n",
    "\n",
    "import nltk\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, matutils\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "os.environ.update({'MALLET_HOME':r'C:/new_mullet/mallet-2.0.8/'})\n",
    "mallet_path = 'C:/new_mullet/mallet-2.0.8/bin/mallet' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening Scrapped Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pokemon.pickle','rb') as read_file:\n",
    "    df = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Group  reviews by their metacritic ratings \n",
    "### 0 to 4 = negative, 5 to 7 mixed, 8 and above = positive \n",
    "\n",
    "def sentiment(x):\n",
    "    if x > 7:\n",
    "        return 'positive'\n",
    "    if x < 5:\n",
    "        return 'negative'\n",
    "    else: return 'mixed'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(lambda x:sentiment(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some users reviewed both pokemon sword and shield. \n",
    "### These reviews tended to be a copy-paste \n",
    "\n",
    "df[df.duplicated('name')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[df['name'] == 'Mack_thge_Sack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove duplicate users \n",
    "df.drop_duplicates(subset='name', keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### detect review language and returns NaN if not english \n",
    "def language_detection(x): \n",
    "    result = detect(x)\n",
    "    if result == 'en':\n",
    "        return x \n",
    "    else: return np.NaN \n",
    "    \n",
    "df['review'] = df['review'].apply(lambda x:language_detection(x))\n",
    "\n",
    "### Only keep english reviews \n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating stopword list from both nltk and spacy \n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stop_words =  list(STOP_WORDS)\n",
    "stop_words.extend(['game','pokemon','pokÃ©mon']) ### these are common words that appear in almost all reviews \n",
    "\n",
    "for word in nltk_stop_words:\n",
    "    if word in stop_words: \n",
    "        continue\n",
    "    else: stop_words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_words(text,wordlist):\n",
    "    for word in wordlist:\n",
    "        if word in text.split():\n",
    "            text = re.sub(r'\\b{}\\b'.format(word), '', text)  \n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub('\\d', ' ', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text) \n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def strip_extraspace(text):\n",
    "    return re.sub('\\s\\s+',' ', text)\n",
    "\n",
    "def replace_word(text,word,replacement):\n",
    "    return text.replace(word,replacement)\n",
    "\n",
    "def remove_r(text):\n",
    "    return text.replace('\\r',' ')\n",
    "#df['review'] = df['review'].apply(lambda x:remove_punctuation(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = make_lower(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_digits(text)\n",
    "    text = replace_word(text,'game freak','gamefreak') ### correcting inconsistencies in spelling that I noticed \n",
    "    text = replace_word(text, 'game play', 'gameplay')\n",
    "    text = remove_words(text,stop_words)\n",
    "    text = remove_r(text)\n",
    "    text = strip_extraspace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lemmatising using spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize_words(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    text = sp(text)\n",
    "    lemmed_string =''\n",
    "    for word in text:\n",
    "        if word.pos_ in allowed_postags:\n",
    "            if word.lemma_ == '-PRON-' or word.lemma_ in stop_words: \n",
    "                ### skip words that are not in allowed postags or becomes a stopword when lemmatised   \n",
    "                continue \n",
    "            else: lemmed_string = lemmed_string+' '+word.lemma_\n",
    "    return lemmed_string.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(lambda x:lemmatize_words(x, allowed_postags=['NOUN', 'VERB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### list all words that appear in less than 4 documents, and remove them from the reviews \n",
    "\n",
    "word_frequency = Counter()\n",
    "\n",
    "for text in df.review:\n",
    "    text = text.split()\n",
    "    word_frequency.update(set(text)) \n",
    "\n",
    "rare_words = []\n",
    "\n",
    "for key, value in word_frequency.items():\n",
    "    if value < 4:\n",
    "        rare_words.append(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(lambda x:remove_words(x,rare_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_freq = sorted(word_frequency.items(), key=lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_freq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting df by sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = df[df['sentiment']=='negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words, ngram_range = (1,2),\n",
    "                                   token_pattern=\"\\\\b[a-z][a-z][a-z]+\\\\b\") ###ignore words that are less than 3 alphabets  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Groundwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(negative.review)\n",
    "doc_word = vectorizer.transform(negative.review).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = dict((v, k) for v, k in vectorizer.vocabulary_.items())\n",
    "id2word = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "\n",
    "dictionary = corpora.Dictionary()\n",
    "dictionary.id2token = id2word\n",
    "dictionary.token2id = word2id\n",
    "\n",
    "texts = negative['review'].apply(lambda x: x.split())\n",
    "texts = texts.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using coherence score to determine the number of topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word, random_seed = 77)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_mlda, coherence_values_mlda = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=texts, start=2, limit=8, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "limit=8; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "\n",
    "plt.plot(x, coherence_values_mlda)\n",
    "\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('CoherenceScore.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=8; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "for m, cv in zip(x, coherence_values_mlda):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = LdaMallet(mallet_path, corpus=corpus, num_topics=5, id2word=id2word, random_seed = 77)\n",
    "\n",
    "    \n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
